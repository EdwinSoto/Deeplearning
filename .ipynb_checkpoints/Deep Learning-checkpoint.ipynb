{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 2.3350 - acc: 0.1013\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 8s 130us/sample - loss: 2.3025 - acc: 0.0987\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 2.3025 - acc: 0.0987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2cb9b1fef28>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = tf.keras.utils.normalize(x_train, axis = 1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis = 1)\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu))\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "model.fit(x_train, y_train, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 38us/sample - loss: 2.3026 - acc: 0.0980\n",
      "2.302585363006592 0.098\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.08216044 0.2286589  0.3728098\n",
      "  0.30506548 0.08583808 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.08087653 0.38341541 0.36240278 0.37133624\n",
      "  0.48350001 0.4068725  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.08861609 0.3824786  0.40758025 0.36240278 0.35218\n",
      "  0.44704564 0.43262392 0.06832372 0.00859123 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.01621743\n",
      "  0.095788   0.36759266 0.42460179 0.40758025 0.36240278 0.29765841\n",
      "  0.16116667 0.43262392 0.30326141 0.17468832 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.26434406\n",
      "  0.4023096  0.41354174 0.42460179 0.40758025 0.36240278 0.37133624\n",
      "  0.18419048 0.32446794 0.30326141 0.23912253 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.08411834 0.38597476\n",
      "  0.40390606 0.41518278 0.32013627 0.18365276 0.36384089 0.33597088\n",
      "  0.09017659 0.13562417 0.30565873 0.2405544  0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.07427511 0.39255225 0.40867916\n",
      "  0.4023096  0.29374592 0.02021913 0.12082418 0.17401086 0.03094469\n",
      "  0.         0.         0.30326141 0.34794476 0.12263192 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.04890249 0.2553207  0.41729294 0.37786605\n",
      "  0.33206506 0.13784725 0.         0.         0.         0.\n",
      "  0.         0.         0.30326141 0.36083161 0.40468535 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.00966301 0.22906954 0.38994434 0.39585101 0.11514373\n",
      "  0.03033287 0.04594908 0.         0.         0.         0.\n",
      "  0.         0.         0.30326141 0.36083161 0.47826451 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.07868449 0.32430069 0.38994434 0.10391089 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30326141 0.36083161 0.47826451 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.27332506 0.3255876  0.29400565 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30565873 0.36226348 0.48071715 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33960736 0.33958568 0.32430069 0.17330859 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30326141 0.36083161 0.3629905  0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.37982402 0.34786826 0.29598873 0.03868495 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01343056 0.23176282 0.30326141 0.26632809 0.02943166 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.37982402 0.34786826 0.28698037 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.0103149\n",
      "  0.25134326 0.43262392 0.26969888 0.10166287 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.37982402 0.34786826 0.18660159 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.0690291  0.24313682\n",
      "  0.48350001 0.29699976 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.38429254 0.34924869 0.28955419 0.         0.         0.\n",
      "  0.         0.         0.         0.18365276 0.34226929 0.3728098\n",
      "  0.31082143 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.37982402 0.34786826 0.32043997 0.22592013 0.0791702  0.04703054\n",
      "  0.13569967 0.29210488 0.37910874 0.40758025 0.3206977  0.24608394\n",
      "  0.10744445 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.37982402 0.34786826 0.32430069 0.38994434 0.37770784 0.34867468\n",
      "  0.4023096  0.41354174 0.42460179 0.31575387 0.18695382 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.1251185  0.27470549 0.32430069 0.38994434 0.41729294 0.40867916\n",
      "  0.4023096  0.38236201 0.24431452 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.03451074 0.16472416 0.38994434 0.41729294 0.40867916\n",
      "  0.2251018  0.06071843 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "\n",
    "plot.imshow(x_train[0], cmap = plot.cm.binary)\n",
    "plot.show()\n",
    "print(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('epic_num_reader.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('epic_num_reader.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-872a9c2f0752>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[1;31m# generate symbolic tensors).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[1;32m-> 1060\u001b[1;33m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[0;32m   1061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2556\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2557\u001b[0m         \u001b[0mcast_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2558\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2559\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2560\u001b[0m       \u001b[0my_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[1;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[0;32m   2774\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_expects_training_arg\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2775\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2776\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2777\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2778\u001b[0m         \u001b[1;31m# This Model or a submodel is dynamic and hasn't overridden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    632\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m                   \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[1;31m# `outputs` will be the inputs to the next layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    632\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m                   \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     outputs = array_ops.reshape(\n\u001b[1;32m--> 580\u001b[1;33m         inputs, (tensor_shape.dimension_value(inputs.shape[0]) or\n\u001b[0m\u001b[0;32m    581\u001b[0m                  array_ops.shape(inputs)[0], -1))\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "predictions = new_model.predict([x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-7ea0ff523e42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
